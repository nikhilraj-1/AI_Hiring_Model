{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c07a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:00<00:00, 9743.15 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 10792.82 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 30:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.958900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.847700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.794400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.857500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.850300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       961\n",
      "           1       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.96      1000\n",
      "   macro avg       0.48      0.50      0.49      1000\n",
      "weighted avg       0.92      0.96      0.94      1000\n",
      "\n",
      "PR AUC (Average Precision): 0.1014730664893092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/nikhilraj/Documents/HiringAiModel/.aihiringvenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Load Data ---\n",
    "X_train = pd.read_csv(\"../data/processed/train.csv\")\n",
    "y_train = pd.read_csv(\"../data/processed/train_labels.csv\").squeeze()\n",
    "X_test = pd.read_csv(\"../data/processed/test.csv\")\n",
    "y_test = pd.read_csv(\"../data/processed/test_labels.csv\").squeeze()\n",
    "\n",
    "# --- Feature Setup ---\n",
    "text_cols = ['candidate_skills', 'past_job_titles', 'certifications', 'required_skills', 'job_description']\n",
    "cat_cols = ['education_level', 'candidate_location', 'job_location', 'job_title']\n",
    "num_cols = [col for col in X_train.columns if col not in text_cols + cat_cols]\n",
    "\n",
    "# --- Prompt Engineering Function ---\n",
    "def create_prompt(row):\n",
    "    parts = []\n",
    "    for col in cat_cols + num_cols + text_cols:\n",
    "        val = str(row[col]) if pd.notna(row[col]) else \"\"\n",
    "        parts.append(f\"{col.replace('_', ' ').title()}: {val}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "X_train['text'] = X_train.apply(create_prompt, axis=1)\n",
    "X_test['text'] = X_test.apply(create_prompt, axis=1)\n",
    "\n",
    "# --- Format for Hugging Face Dataset ---\n",
    "train_df = pd.DataFrame({'text': X_train['text'], 'label': y_train})\n",
    "test_df = pd.DataFrame({'text': X_test['text'], 'label': y_test})\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# --- Compute Class Weights ---\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# --- Custom Loss Function ---\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedDistilBERT(nn.Module):\n",
    "    def __init__(self, base_model, class_weights):\n",
    "        super(WeightedDistilBERT, self).__init__()\n",
    "        self.model = base_model\n",
    "        self.loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=None)\n",
    "        logits = outputs.logits\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# --- Load Base Model and Wrap ---\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model = WeightedDistilBERT(base_model, class_weights_tensor)\n",
    "\n",
    "# --- Training Arguments (v4.5.2 compatible) ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Predict ---\n",
    "preds = trainer.predict(test_ds)\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"\\nDistilBERT Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"PR AUC (Average Precision):\", average_precision_score(y_test, preds.predictions[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92957a0",
   "metadata": {},
   "source": [
    "\n",
    "#Conclusion\n",
    "Although the overall accuracy is high (96%), this is misleading due to severe class imbalance.\n",
    "\n",
    "The model completely fails to identify the minority class (label 1) — with 0 precision and 0 recall, meaning not a single positive case was predicted.\n",
    "\n",
    "This results in a macro average F1-score of just 0.49, indicating poor generalization across both classes.\n",
    "\n",
    "Despite prompt engineering and class weighting, the model defaults to majority class predictions.\n",
    "\n",
    "This version is not suitable for deployment in scenarios where correctly detecting the minority class (e.g. qualified candidates, fraud, defects) is critical.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aihiringvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
